#!/usr/bin/env python3
"""
TS2Vec vs T-Rep ë ˆì´ë¸” ì¶”ë¡  ëŠ¥ë ¥ ë¹„êµ (ì´ìƒ íƒì§€ ì„±ëŠ¥)

ëª©í‘œ: class=3 (ë² ì´ìŠ¤ë¼ì¸ ì´ìƒ) ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì°¾ì•„ë‚´ëŠ”ê°€?

ë°©ë²•:
1. TEST ë°ì´í„°ì˜ class ë ˆì´ë¸”ì„ ìˆ¨ê¹€
2. ChromaDBì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ kê°œì˜ ì´ì›ƒ ê²€ìƒ‰
3. ì´ì›ƒë“¤ì˜ classë¡œ ì˜ˆì¸¡ (majority voting)
4. ì‹¤ì œ classì™€ ë¹„êµí•˜ì—¬ ì •í™•ë„ ì¸¡ì •
"""

import sys
import os
sys.path.insert(0, '/home/0_code/NIER_Pipelines')

import chromadb
from chromadb.config import Settings
import numpy as np
import pandas as pd
import json
from tqdm import tqdm
import logging
from datetime import datetime
from collections import Counter
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)

from NIERModules.chroma_ts2vec import Ts2VecEmbedding
from NIERModules.chroma_trep import TRepEmbedding

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ì „ì—­ ì„¤ì •
DB_PATH = "/home/0_code/RAG/ChromaDB/ts2vec_embedding_function/NIER_DB"
TEST_DATA_PATH = "/home/0_code/RAG/ts2vec/datasets/NIER/NIER_v2_TEST.csv"
ELEMENTS = ["SO2", "O3", "CO", "NO", "NO2"]
DEVICE = "cuda"

# ëª¨ë¸ ê²½ë¡œ
TS2VEC_MODEL_PATHS = {
    "SO2": "/home/0_code/RAG/ts2vec/training/v0_1_1__v0_1_1_SO2_20250427_213952/model.pkl",
    "O3": "/home/0_code/RAG/ts2vec/training/v0_1_1__v0_1_1_O3_20250427_213507/model.pkl",
    "CO": "/home/0_code/RAG/ts2vec/training/v0_1_1__v0_1_1_CO_20250427_210220/model.pkl",
    "NO": "/home/0_code/RAG/ts2vec/training/v0_1_1__v0_1_1_NO_20250427_212025/model.pkl",
    "NO2": "/home/0_code/RAG/ts2vec/training/v0_1_1__v0_1_1_NO2_20250427_212740/model.pkl"
}

TREP_MODEL_PATHS = {
    "SO2": "/home/0_code/NIER_Pipelines/NIERModules/chroma_trep/model_pkl/model_SO2.pt",
    "O3": "/home/0_code/NIER_Pipelines/NIERModules/chroma_trep/model_pkl/model_O3.pt",
    "CO": "/home/0_code/NIER_Pipelines/NIERModules/chroma_trep/model_pkl/model_CO.pt",
    "NO": "/home/0_code/NIER_Pipelines/NIERModules/chroma_trep/model_pkl/model_NO.pt",
    "NO2": "/home/0_code/NIER_Pipelines/NIERModules/chroma_trep/model_pkl/model_NO2.pt"
}


class ClassificationBenchmark:
    def __init__(self, k_neighbors=5):
        self.k_neighbors = k_neighbors
        
        self.client = chromadb.PersistentClient(
            path=DB_PATH,
            settings=Settings(anonymized_telemetry=False)
        )
        
        self.ts2vec_collection = self.client.get_collection("time_series_collection")
        self.trep_collection = self.client.get_collection("time_series_collection_trep")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ: {TEST_DATA_PATH}")
        self.test_data = pd.read_csv(TEST_DATA_PATH)
        
        # class ì—´ì´ ìˆëŠ”ì§€ í™•ì¸
        if 'class' not in self.test_data.columns:
            raise ValueError("TEST ë°ì´í„°ì— 'class' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        
        logger.info(f"  ì „ì²´ ë ˆì½”ë“œ: {len(self.test_data)}")
        logger.info(f"  í´ë˜ìŠ¤ ë¶„í¬:")
        class_counts = self.test_data['class'].value_counts()
        for cls, count in sorted(class_counts.items()):
            logger.info(f"    class={cls}: {count}ê°œ")
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.ts2vec_models = self._init_ts2vec_models()
        self.trep_models = self._init_trep_models()
        
        # ê²°ê³¼ ì €ì¥
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "config": {
                "test_data": TEST_DATA_PATH,
                "k_neighbors": k_neighbors,
                "device": DEVICE
            },
            "ts2vec": {},
            "trep": {},
            "comparison": {}
        }
    
    def _init_ts2vec_models(self):
        """TS2Vec ëª¨ë¸ ì´ˆê¸°í™”"""
        logger.info("TS2Vec ëª¨ë¸ ì´ˆê¸°í™”...")
        models = {}
        for elem in ELEMENTS:
            models[elem] = Ts2VecEmbedding(
                weight_path=TS2VEC_MODEL_PATHS[elem],
                device=DEVICE,
                encoding_window='full_series'
            )
        logger.info("TS2Vec ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        return models
    
    def _init_trep_models(self):
        """T-Rep ëª¨ë¸ ì´ˆê¸°í™”"""
        logger.info("T-Rep ëª¨ë¸ ì´ˆê¸°í™”...")
        models = {}
        for elem in ELEMENTS:
            models[elem] = TRepEmbedding(
                weight_path=TREP_MODEL_PATHS[elem],
                device=DEVICE,
                encoding_window='full_series',
                time_embedding='t2v_sin'
            )
        logger.info("T-Rep ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        return models
    
    def predict_class_knn(self, query_embedding, collection, element, k):
        """
        K-NNì„ ì‚¬ìš©í•˜ì—¬ í´ë˜ìŠ¤ ì˜ˆì¸¡
        
        Args:
            query_embedding: ì¿¼ë¦¬ ì„ë² ë”©
            collection: ChromaDB collection
            element: ì›ì†Œ (SO2, O3, ...)
            k: ì´ì›ƒ ìˆ˜
        
        Returns:
            predicted_class: ì˜ˆì¸¡ëœ í´ë˜ìŠ¤
            neighbor_classes: ì´ì›ƒë“¤ì˜ í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ê°€ì¥ ê°€ê¹Œìš´ kê°œ ê²€ìƒ‰
            results = collection.query(
                query_embeddings=[query_embedding.tolist()],
                n_results=k,
                where={"element": element},
                include=['metadatas']
            )
            
            if not results['metadatas'] or not results['metadatas'][0]:
                return None, []
            
            # ì´ì›ƒë“¤ì˜ í´ë˜ìŠ¤ ì¶”ì¶œ
            neighbor_classes = []
            for metadata in results['metadatas'][0]:
                if 'class' in metadata:
                    try:
                        cls = int(metadata['class'])
                        neighbor_classes.append(cls)
                    except (ValueError, TypeError):
                        continue
            
            if not neighbor_classes:
                return None, []
            
            # Majority voting
            class_counts = Counter(neighbor_classes)
            predicted_class = class_counts.most_common(1)[0][0]
            
            return predicted_class, neighbor_classes
            
        except Exception as e:
            logger.error(f"ì˜ˆì¸¡ ì—ëŸ¬: {e}")
            return None, []
    
    def benchmark_classification(self, model_name, models, collection, max_samples_per_element=100):
        """
        ë¶„ë¥˜ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
        
        Args:
            model_name: "TS2Vec" ë˜ëŠ” "T-Rep"
            models: ëª¨ë¸ ë”•ì…”ë„ˆë¦¬
            collection: ChromaDB collection
            max_samples_per_element: ì›ì†Œë‹¹ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜
        """
        logger.info("=" * 60)
        logger.info(f"{model_name} ë ˆì´ë¸” ì¶”ë¡  ëŠ¥ë ¥ ë²¤ì¹˜ë§ˆí¬ (k={self.k_neighbors})")
        logger.info("=" * 60)
        
        all_predictions = []
        all_true_labels = []
        element_results = {}
        
        for elem in ELEMENTS:
            logger.info(f"\nì›ì†Œ: {elem}")
            
            # í•´ë‹¹ ì›ì†Œì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ
            elem_data = self.test_data[self.test_data['element'] == elem].copy()
            
            if len(elem_data) == 0:
                logger.warning(f"  {elem}: í…ŒìŠ¤íŠ¸ ë°ì´í„° ì—†ìŒ")
                continue
            
            # ìƒ˜í”Œ ìˆ˜ ì œí•œ
            if len(elem_data) > max_samples_per_element:
                elem_data = elem_data.sample(n=max_samples_per_element, random_state=42)
            
            logger.info(f"  í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(elem_data)}")
            
            predictions = []
            true_labels = []
            
            for idx, row in tqdm(elem_data.iterrows(), total=len(elem_data), 
                                desc=f"{model_name} {elem}", leave=False):
                try:
                    # ì„ë² ë”© ìƒì„±
                    embedding = models[elem]([row['values']])[0]
                    
                    # í´ë˜ìŠ¤ ì˜ˆì¸¡
                    pred_class, neighbor_classes = self.predict_class_knn(
                        embedding, collection, elem, self.k_neighbors
                    )
                    
                    if pred_class is not None:
                        predictions.append(pred_class)
                        true_labels.append(int(row['class']))
                    
                except Exception as e:
                    logger.error(f"ìƒ˜í”Œ ì²˜ë¦¬ ì—ëŸ¬ ({idx}): {e}")
                    continue
            
            if len(predictions) > 0:
                # ì›ì†Œë³„ ì„±ëŠ¥ ê³„ì‚°
                accuracy = accuracy_score(true_labels, predictions)
                
                # class=3 (ì´ìƒ)ì— ëŒ€í•œ Recall (ê°€ì¥ ì¤‘ìš”!)
                recall_class3 = recall_score(true_labels, predictions, labels=[3], average='macro', zero_division=0)
                
                # ì „ì²´ precision, recall, f1
                precision = precision_score(true_labels, predictions, average='macro', zero_division=0)
                recall = recall_score(true_labels, predictions, average='macro', zero_division=0)
                f1 = f1_score(true_labels, predictions, average='macro', zero_division=0)
                
                # Confusion matrix
                cm = confusion_matrix(true_labels, predictions)
                
                element_results[elem] = {
                    "accuracy": float(accuracy),
                    "precision": float(precision),
                    "recall": float(recall),
                    "f1_score": float(f1),
                    "recall_class3": float(recall_class3),
                    "confusion_matrix": cm.tolist(),
                    "n_samples": len(predictions)
                }
                
                logger.info(f"  ì •í™•ë„: {accuracy:.4f}")
                logger.info(f"  Precision: {precision:.4f}")
                logger.info(f"  Recall: {recall:.4f}")
                logger.info(f"  F1-Score: {f1:.4f}")
                logger.info(f"  â­ class=3 Recall: {recall_class3:.4f} (ì´ìƒ íƒì§€ ì„±ëŠ¥)")
                
                all_predictions.extend(predictions)
                all_true_labels.extend(true_labels)
        
        # ì „ì²´ ì„±ëŠ¥ ê³„ì‚°
        if len(all_predictions) > 0:
            overall_accuracy = accuracy_score(all_true_labels, all_predictions)
            overall_precision = precision_score(all_true_labels, all_predictions, average='macro', zero_division=0)
            overall_recall = recall_score(all_true_labels, all_predictions, average='macro', zero_division=0)
            overall_f1 = f1_score(all_true_labels, all_predictions, average='macro', zero_division=0)
            overall_recall_class3 = recall_score(all_true_labels, all_predictions, labels=[3], average='macro', zero_division=0)
            
            # Classification report
            report = classification_report(all_true_labels, all_predictions, zero_division=0, output_dict=True)
            
            logger.info(f"\n{'='*60}")
            logger.info(f"{model_name} ì „ì²´ ì„±ëŠ¥")
            logger.info(f"{'='*60}")
            logger.info(f"ì „ì²´ ì •í™•ë„: {overall_accuracy:.4f}")
            logger.info(f"ì „ì²´ Precision: {overall_precision:.4f}")
            logger.info(f"ì „ì²´ Recall: {overall_recall:.4f}")
            logger.info(f"ì „ì²´ F1-Score: {overall_f1:.4f}")
            logger.info(f"â­ class=3 Recall: {overall_recall_class3:.4f} (ì´ìƒ íƒì§€ ì„±ëŠ¥)")
            
            overall_results = {
                "accuracy": float(overall_accuracy),
                "precision": float(overall_precision),
                "recall": float(overall_recall),
                "f1_score": float(overall_f1),
                "recall_class3": float(overall_recall_class3),
                "classification_report": report,
                "n_samples": len(all_predictions),
                "by_element": element_results
            }
            
            return overall_results
        
        return None
    
    def run_benchmark(self, max_samples_per_element=100):
        """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        logger.info("ğŸš€ ë ˆì´ë¸” ì¶”ë¡  ëŠ¥ë ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        logger.info(f"ë‚ ì§œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"k-neighbors: {self.k_neighbors}")
        logger.info(f"ì›ì†Œ: {', '.join(ELEMENTS)}")
        logger.info("")
        
        try:
            # TS2Vec ë²¤ì¹˜ë§ˆí¬
            ts2vec_results = self.benchmark_classification(
                "TS2Vec", 
                self.ts2vec_models, 
                self.ts2vec_collection,
                max_samples_per_element
            )
            if ts2vec_results:
                self.results["ts2vec"] = ts2vec_results
            
            # T-Rep ë²¤ì¹˜ë§ˆí¬
            trep_results = self.benchmark_classification(
                "T-Rep", 
                self.trep_models, 
                self.trep_collection,
                max_samples_per_element
            )
            if trep_results:
                self.results["trep"] = trep_results
            
            # ë¹„êµ
            if ts2vec_results and trep_results:
                self.results["comparison"] = {
                    "accuracy_improvement": float(trep_results["accuracy"] - ts2vec_results["accuracy"]),
                    "f1_improvement": float(trep_results["f1_score"] - ts2vec_results["f1_score"]),
                    "recall_class3_improvement": float(trep_results["recall_class3"] - ts2vec_results["recall_class3"]),
                }
                
                logger.info("\n" + "=" * 60)
                logger.info("ğŸ“Š ë¹„êµ ê²°ê³¼")
                logger.info("=" * 60)
                logger.info(f"ì •í™•ë„ í–¥ìƒ: {self.results['comparison']['accuracy_improvement']:+.4f}")
                logger.info(f"F1-Score í–¥ìƒ: {self.results['comparison']['f1_improvement']:+.4f}")
                logger.info(f"â­ class=3 Recall í–¥ìƒ: {self.results['comparison']['recall_class3_improvement']:+.4f}")
            
        except Exception as e:
            logger.error(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {e}")
            import traceback
            traceback.print_exc()
        
        return self.results
    
    def save_results(self, output_path="/home/0_code/NIER_Pipelines/scripts/classification_benchmark_results.json"):
        """ê²°ê³¼ ì €ì¥"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        logger.info(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_path}")
    
    def generate_report(self, output_path="/home/0_code/NIER_Pipelines/scripts/classification_benchmark_report.md"):
        """ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±"""
        ts2vec = self.results.get("ts2vec", {})
        trep = self.results.get("trep", {})
        comparison = self.results.get("comparison", {})
        
        report = f"""# TS2Vec vs T-Rep ë ˆì´ë¸” ì¶”ë¡  ëŠ¥ë ¥ ë¹„êµ (ì´ìƒ íƒì§€ ì„±ëŠ¥)

**ìƒì„± ì¼ì‹œ**: {self.results['timestamp']}  
**k-neighbors**: {self.k_neighbors}  
**í…ŒìŠ¤íŠ¸ ë°ì´í„°**: {TEST_DATA_PATH}  
**í…ŒìŠ¤íŠ¸ ì›ì†Œ**: {', '.join(ELEMENTS)}

---

## 1. ì „ì²´ ì„±ëŠ¥ ë¹„êµ

| ì§€í‘œ | TS2Vec | T-Rep | í–¥ìƒ |
|------|--------|-------|------|
"""
        
        if ts2vec and trep:
            report += f"| ì •í™•ë„ (Accuracy) | {ts2vec['accuracy']:.4f} | {trep['accuracy']:.4f} | {comparison['accuracy_improvement']:+.4f} |\n"
            report += f"| Precision | {ts2vec['precision']:.4f} | {trep['precision']:.4f} | - |\n"
            report += f"| Recall | {ts2vec['recall']:.4f} | {trep['recall']:.4f} | - |\n"
            report += f"| F1-Score | {ts2vec['f1_score']:.4f} | {trep['f1_score']:.4f} | {comparison['f1_improvement']:+.4f} |\n"
            report += f"| **â­ class=3 Recall** | **{ts2vec['recall_class3']:.4f}** | **{trep['recall_class3']:.4f}** | **{comparison['recall_class3_improvement']:+.4f}** |\n"
        
        report += "\n**class=3 Recall**: ë² ì´ìŠ¤ë¼ì¸ ì´ìƒ(ì´ìƒ ë°ì´í„°)ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì°¾ì•„ë‚´ëŠ”ê°€? (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)\n"
        
        report += "\n---\n\n## 2. ì›ì†Œë³„ ì„±ëŠ¥ ë¹„êµ\n\n"
        
        if ts2vec and trep and "by_element" in ts2vec and "by_element" in trep:
            report += "### 2.1 ì •í™•ë„ (Accuracy)\n\n"
            report += "| ì›ì†Œ | TS2Vec | T-Rep | í–¥ìƒ |\n|------|--------|-------|------|\n"
            
            for elem in ELEMENTS:
                if elem in ts2vec["by_element"] and elem in trep["by_element"]:
                    ts2vec_acc = ts2vec["by_element"][elem]["accuracy"]
                    trep_acc = trep["by_element"][elem]["accuracy"]
                    improvement = trep_acc - ts2vec_acc
                    report += f"| {elem} | {ts2vec_acc:.4f} | {trep_acc:.4f} | {improvement:+.4f} |\n"
            
            report += "\n### 2.2 class=3 Recall (ì´ìƒ íƒì§€)\n\n"
            report += "| ì›ì†Œ | TS2Vec | T-Rep | í–¥ìƒ |\n|------|--------|-------|------|\n"
            
            for elem in ELEMENTS:
                if elem in ts2vec["by_element"] and elem in trep["by_element"]:
                    ts2vec_recall = ts2vec["by_element"][elem]["recall_class3"]
                    trep_recall = trep["by_element"][elem]["recall_class3"]
                    improvement = trep_recall - ts2vec_recall
                    report += f"| {elem} | {ts2vec_recall:.4f} | {trep_recall:.4f} | {improvement:+.4f} |\n"
            
            report += "\n### 2.3 F1-Score\n\n"
            report += "| ì›ì†Œ | TS2Vec | T-Rep | í–¥ìƒ |\n|------|--------|-------|------|\n"
            
            for elem in ELEMENTS:
                if elem in ts2vec["by_element"] and elem in trep["by_element"]:
                    ts2vec_f1 = ts2vec["by_element"][elem]["f1_score"]
                    trep_f1 = trep["by_element"][elem]["f1_score"]
                    improvement = trep_f1 - ts2vec_f1
                    report += f"| {elem} | {ts2vec_f1:.4f} | {trep_f1:.4f} | {improvement:+.4f} |\n"
        
        report += "\n---\n\n## 3. ê²°ë¡ \n\n"
        
        if comparison:
            if comparison["recall_class3_improvement"] > 0:
                report += f"### âœ… T-Repì´ ì´ìƒ íƒì§€ì—ì„œ ìš°ìˆ˜\n\n"
                report += f"- class=3 Recallì´ **{comparison['recall_class3_improvement']:+.4f}** í–¥ìƒ\n"
                report += f"- ë² ì´ìŠ¤ë¼ì¸ ì´ìƒ ë°ì´í„°ë¥¼ ë” ì˜ ì°¾ì•„ëƒ„\n"
            else:
                report += f"### âš ï¸ TS2Vecì´ ì´ìƒ íƒì§€ì—ì„œ ìš°ìˆ˜\n\n"
                report += f"- class=3 Recallì´ **{comparison['recall_class3_improvement']:+.4f}** ë³€í™”\n"
            
            if comparison["accuracy_improvement"] > 0:
                report += f"- ì „ì²´ ì •í™•ë„ë„ **{comparison['accuracy_improvement']:+.4f}** í–¥ìƒ\n"
            
            if comparison["f1_improvement"] > 0:
                report += f"- F1-Scoreë„ **{comparison['f1_improvement']:+.4f}** í–¥ìƒ\n"
        
        report += "\n### ì£¼ìš” ë°œê²¬\n\n"
        report += "ì´ ë²¤ì¹˜ë§ˆí¬ëŠ” **ê²€ìƒ‰ ê¸°ë°˜ ë¶„ë¥˜(k-NN)** ë°©ì‹ìœ¼ë¡œ ë ˆì´ë¸”ì„ ì¶”ë¡ í•©ë‹ˆë‹¤.\n"
        report += "- ë†’ì€ class=3 Recall = ì´ìƒ ë°ì´í„°ë¥¼ ì˜ íƒì§€\n"
        report += "- ë†’ì€ ì •í™•ë„ = ì „ì²´ì ìœ¼ë¡œ ë ˆì´ë¸”ì„ ì˜ ì˜ˆì¸¡\n"
        report += "- F1-Score = Precisionê³¼ Recallì˜ ì¡°í™”í‰ê· \n"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"âœ… ë¦¬í¬íŠ¸ ì €ì¥: {output_path}")


if __name__ == "__main__":
    logger.info("=" * 60)
    logger.info("TS2Vec vs T-Rep ë ˆì´ë¸” ì¶”ë¡  ëŠ¥ë ¥ ë²¤ì¹˜ë§ˆí¬")
    logger.info("=" * 60)
    
    try:
        # k=5ë¡œ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        benchmark = ClassificationBenchmark(k_neighbors=5)
        results = benchmark.run_benchmark(max_samples_per_element=100)
        
        # ê²°ê³¼ ì €ì¥
        benchmark.save_results()
        benchmark.generate_report()
        
        logger.info("\n" + "=" * 60)
        logger.info("âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
